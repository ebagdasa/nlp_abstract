{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import parser\n",
    "import numpy as np\n",
    "import gzip, msgpack\n",
    "from torch.nn.modules import PairwiseDistance\n",
    "import random\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import scipy.sparse as sp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "N_FEATURES = 2**18\n",
    "hv = HashingVectorizer(n_features=N_FEATURES,non_negative=True, stop_words='english')\n",
    "# try default n-gram 1-gram,2-gram,3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SmallReference:\n",
    "    def __init__(self, identifiers):\n",
    "        self.identifiers = identifiers\n",
    "\n",
    "class SmallPaper:\n",
    "    def __init__(self, title, identifiers, authors, abstract, refs):\n",
    "        self.title = title\n",
    "        self.authors = authors\n",
    "        self.abstract = hv.transform([abstract])\n",
    "        self.references = [SmallReference(ref.identifiers) for ref in refs]\n",
    "        self.identifiers = identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doi(paper):\n",
    "    _ref_dict = dict()\n",
    "    for x in paper.identifiers:\n",
    "#        @todo lowecase\n",
    "        _ref_dict[x.key_type] = x.key.lower()\n",
    "    if 'doi' in _ref_dict:\n",
    "        return _ref_dict['doi']\n",
    "    elif 'ieee' in _ref_dict:\n",
    "        return _ref_dict['ieee']\n",
    "    elif 'semanticscholar' in _ref_dict:\n",
    "        return _ref_dict['semanticscholar']\n",
    "    elif 'adsa' in _ref_dict:\n",
    "        return _ref_dict['adsa']\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def update_authors(ident_to_auth, authors_to_papers, paper):\n",
    "    for author in paper.authors:\n",
    "        author_name = author.name.name\n",
    "        \n",
    "        if not author_name:\n",
    "            continue\n",
    "        for identifier in author.identifiers:\n",
    "            result = ident_to_auth.get(identifier.key.lower(), False) \n",
    "            if result:\n",
    "                author_name = str(result)\n",
    "                break\n",
    "        for identifier in author.identifiers:\n",
    "            ident_to_auth[identifier.key.lower()] = author_name\n",
    "        if not authors_to_papers.get(author_name, False):\n",
    "            authors_to_papers[author_name] = list()\n",
    "        authors_to_papers[author_name].append(get_doi(paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:adsa:2000JMatS..35.4393W title is empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total count: 14456\n",
      "{'adsa': 2260, 'ieee': 3468, 'iop': 189, 'onepetro': 45, 'arxiv': 520, 'pmc': 361, 'acm': 1904, 'doi': 7638, 'oxford': 211, 'mid': 23, 'semanticscholar': 2702, 'science': 5, 'springer': 856, 'sage': 38, 'pmid': 1310, 'nature': 105, 'manuscript': 47}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "identies_set = dict()\n",
    "\n",
    "_paper_dict = dict()\n",
    "myiter = 0\n",
    "count=0\n",
    "_ref_dict = dict()\n",
    "_training_set = list()\n",
    "_validation_set = list()\n",
    "_test_set = list()\n",
    "_ident_to_auth_dict = dict()\n",
    "_authors_to_papers_dict = dict()\n",
    "with gzip.open(\"NN_Papers.msgpack.gz\", \"rb\") as nn_papers_out:\n",
    "    unpacker = msgpack.Unpacker(nn_papers_out, encoding='utf-8')\n",
    "    for _paper in unpacker:\n",
    "        count+=1\n",
    "        paper = parser.Paper.deserialize(_paper)\n",
    "        \n",
    "        paper_doi = get_doi(paper)\n",
    "        if not (paper_doi and paper.abstract):\n",
    "            continue\n",
    "        else:\n",
    "            myiter+=1\n",
    "        paper = SmallPaper(paper.title, paper.identifiers, paper.authors, paper.abstract, paper.references)\n",
    "        if myiter>=10000:\n",
    "            break\n",
    "        #calculate statistics\n",
    "        for iden in paper.identifiers:\n",
    "            if identies_set.get(iden.key_type, False):\n",
    "                identies_set[iden.key_type]+=1\n",
    "            else:\n",
    "                identies_set[iden.key_type]=1\n",
    "        #end\n",
    "        \n",
    "        _ref_dict[paper_doi] = list()\n",
    " \n",
    "        update_authors(_ident_to_auth_dict, _authors_to_papers_dict, paper)\n",
    "        _paper_dict[paper_doi] = paper\n",
    "        if myiter<=160000:\n",
    "            _training_set.append(paper_doi)\n",
    "        elif myiter<=180000: \n",
    "            _test_set.append(paper_doi)\n",
    "        else:\n",
    "            _validation_set.append(paper_doi)\n",
    "\n",
    "print('total count: {0}'.format(count))\n",
    "print(identies_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1033\n"
     ]
    }
   ],
   "source": [
    "def update_references(paper_dict, ref_dict):\n",
    "    count = 0\n",
    "    for paper_doi,paper in paper_dict.items():\n",
    "        for ref in paper.references:\n",
    "            ref_doi = get_doi(ref)\n",
    "            if not ref_doi:\n",
    "                continue\n",
    "            result = ref_dict.get(ref_doi, False)\n",
    "            if result!=False:\n",
    "                count+=1\n",
    "                ref_dict[paper_doi].append(ref_doi)\n",
    "    print(count)\n",
    "update_references(_paper_dict, _ref_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list(_paper_.keys())[6]\n",
    "# get_doi(_paper_dict['10.1109/tnnls.2015.2392563'].references[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# abstract_training_set = [paper.abstract for paper in training_set]\n",
    "# abstract_training_set_hashed = hv.transform(abstract_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "\n",
    "def make_var(abstract):\n",
    "    dense = abstract.todense()\n",
    "    torch_tensor = Variable(torch.from_numpy(dense).type(dtype))\n",
    "    return torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# random.randint(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# identies_set = dict()\n",
    "# for paper in paper_list.values():\n",
    "#     for iden in paper.identifiers:\n",
    "#         if identies_set.get(iden.key_type, False):\n",
    "#             identies_set[iden.key_type]+=1\n",
    "#         else:\n",
    "#             identies_set[iden.key_type]=1\n",
    "# print(identies_set)\n",
    "# len(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def get_triple(val_set):\n",
    "#     count = 0\n",
    "#     t0 = 0\n",
    "#     t1 =0 b\n",
    "#     for item in val_set:\n",
    "#         count += 1\n",
    "#         _ref_doi = get_doi(item)\n",
    "#         if _ref_doi not in ref_dict.keys():\n",
    "#             continue\n",
    "#         _ref_list = ref_dict[_ref_doi]\n",
    "        \n",
    "#         for _ref in _ref_list:\n",
    "#             if paper_list.get(_ref, False) in val_set:\n",
    "#                 _neg_example = None\n",
    "#                 while(not _neg_example):\n",
    "#                     _neg_id = random.randint(0,len(val_set)-1)\n",
    "#                     _neg_doi = get_doi(val_set[_neg_id])\n",
    "#                     if (_neg_doi != _ref_doi) and (_neg_doi not in _ref_list):\n",
    "#                         neg_ref_dict = ref_dict[_neg_doi]\n",
    "#                         if not (_ref_doi in neg_ref_dict or _ref in neg_ref_dict):\n",
    "#                             _neg_example = val_set[_neg_id]\n",
    "# #                 t1 = time.time()\n",
    "# #                 print('time to sample triple: {0}'.format(t1-t0))\n",
    "#                 yield (item.abstract, paper_list[_ref].abstract, _neg_example.abstract)\n",
    "# #                 t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# appr = _paper_dict['10.1109/tnnls.2015.2392563']\n",
    "def get_papers_per_authors(paper):\n",
    "    set_to_merge = set()\n",
    "    for author in paper.authors:\n",
    "        identifier = list(author.identifiers)[0].key\n",
    "#         print(\"name {0}. papers: {1}\".format(_ident_to_auth_dict[identifier],\n",
    "#                                              _authors_to_papers_dict[_ident_to_auth_dict[identifier]]))\n",
    "        set_to_merge.update(set(_authors_to_papers_dict[_ident_to_auth_dict[identifier.lower()]]))\n",
    "    set_to_merge.discard(get_doi(paper))\n",
    "    return set_to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_papers_per_authors(_paper_dict['10.1109/tnnls.2015.2392563'])\n",
    "# [x for x in (1,2,3,4)]\n",
    "{1,3,4,5}.intersection([4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "def prepare_triples(val_set):\n",
    "    count = 0\n",
    "    result_list = list()\n",
    "    batch_list_1 = None\n",
    "    batch_list_2 = None\n",
    "    batch_list_3 = None\n",
    "#     print(len(val_set))\n",
    "    for paper_doi in val_set:\n",
    "        paper = _paper_dict[paper_doi]\n",
    "        \n",
    "        \n",
    "        pos_ref_set = get_papers_per_authors(paper).intersection(val_set)\n",
    "        \n",
    "        pos_ref_set.update(set(_ref_dict[paper_doi]).intersection(val_set))\n",
    "        #cut down articles that are not used in val_set\n",
    "        \n",
    "#         pos_ref_set.intersection(val_set)\n",
    "        \n",
    "        if len(pos_ref_set) == 0:\n",
    "#             print(paper_doi)\n",
    "            continue\n",
    "#         print(len(pos_ref_set))\n",
    "        neg_ref_set = set()\n",
    "        for n in range(len(pos_ref_set)):\n",
    "            new_id = None\n",
    "            while((new_id in pos_ref_set) or new_id==None):\n",
    "                new_id = val_set[random.randrange(0,len(val_set)-1)]\n",
    "            neg_ref_set.add(new_id)\n",
    "            \n",
    "            \n",
    "        for pos,neg in zip(pos_ref_set, neg_ref_set):\n",
    "\n",
    "            if batch_list_1 == None:\n",
    "                batch_list_1 = paper.abstract\n",
    "                batch_list_2 = _paper_dict[pos].abstract\n",
    "                batch_list_3 = _paper_dict[neg].abstract\n",
    "            else:\n",
    "                batch_list_1 = sp.vstack((batch_list_1, paper.abstract))\n",
    "                batch_list_2 = sp.vstack((batch_list_2, _paper_dict[pos].abstract))\n",
    "                batch_list_3 = sp.vstack((batch_list_3, _paper_dict[neg].abstract))\n",
    "\n",
    "            if batch_list_1.shape[0]==BATCH_SIZE:\n",
    "                yield (batch_list_1, batch_list_2, batch_list_3)\n",
    "                batch_list_1 = None\n",
    "#     print(count)\n",
    "#     return result_list\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# get_papers_per_authors(_paper_dict[_training_set[2]]).intersection(_training_set[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a1,a2,a3 = next(triples_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# dtype = torch.FloatTensor\n",
    "# def get_batch(iterator, batch_size=BATCH_SIZE):\n",
    "#     a1 = None\n",
    "#     a2 = None\n",
    "#     a3 = None\n",
    "#     for x1,x2,x3 in iterator:\n",
    "#         if a1 == None:\n",
    "#             a1=x1\n",
    "#             a2=x2\n",
    "#             a3=x3\n",
    "#         else:\n",
    "#             a1 = sp.vstack((a1,x1))\n",
    "#             a2 = sp.vstack((a2,x2))\n",
    "#             a3 = sp.vstack((a3,x3))            \n",
    "#         if a1.shape[0]==batch_size:\n",
    "#             dense_a1 = a1.todense()\n",
    "#             dense_a2 = a2.todense()\n",
    "#             dense_a3 = a3.todense()\n",
    "#             tor_a1 = torch.from_numpy(dense_a1).type(dtype)\n",
    "#             tor_a2 = torch.from_numpy(dense_a2).type(dtype)\n",
    "#             tor_a3 = torch.from_numpy(dense_a3).type(dtype)\n",
    "#             yield (Variable(tor_a1),\n",
    "#                    Variable(tor_a2),\n",
    "#                    Variable(tor_a3))\n",
    "#             a1 = None\n",
    "#             a2 = None\n",
    "#             a3 = None\n",
    "\n",
    "\n",
    "# input data sparse matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hv.transform(['avasd', 'asdfwe sdfasdfwe werwe adsfsd']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistRes2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistRes, self).__init__()\n",
    "        self.layer1 = nn.Linear(N_FEATURES, 100)\n",
    "        self.layer2 = nn.PairwiseDistance(2)\n",
    "\n",
    "    def forward(self, x1,x2):\n",
    "#         print('typex: {0}'.format(type(x1)))\n",
    "#         print('typey: {0}'.format(type(x2)))\n",
    "        res1 = self.layer1(x1)\n",
    "        res2 = self.layer1(x2)\n",
    "        # normalize vector L2-normalization\n",
    "        return self.layer2(res1,res2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistRes(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistRes, self).__init__()\n",
    "        self.layer1 = nn.Linear(N_FEATURES, 100)\n",
    "        self.layer2 = nn.PairwiseDistance(2)\n",
    "\n",
    "    def forward(self, x1,x2):\n",
    "#         print('typex: {0}'.format(type(x1)))\n",
    "#         print('typey: {0}'.format(type(x2)))\n",
    "        res1 = self.layer1(x1)\n",
    "        res2 = self.layer1(x2)\n",
    "        # normalize vector L2-normalization\n",
    "        return self.layer2(res1,res2)\n",
    "    \n",
    "    def calculate_res(self, x):\n",
    "        return self.layer1(x).data.numpy()\n",
    "    \n",
    "# linear/RelU/linear\n",
    "# dropout input layer\n",
    "# help as they authored by same person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dist_model = DistRes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check the distance Loss, just to see the difference\n",
    "# maybe pos and pos close\n",
    "\n",
    "crit = nn.HingeEmbeddingLoss(1)\n",
    "optim = torch.optim.SGD(dist_model.parameters(), lr=1e-1)\n",
    "# local_dist(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones_var = Variable(torch.ones(BATCH_SIZE))\n",
    "neg_var = Variable(torch.ones(BATCH_SIZE)*(-1))\n",
    "\n",
    "def train_networks(model, criteria, optimizer, paper, p_pos, p_neg):\n",
    "                    \n",
    "    loss_pos = criteria.forward( model(paper, p_pos), ones_var)\n",
    "    optimizer.zero_grad()\n",
    "    gradCrit = loss_pos.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_neg = criteria.forward( model(paper,p_neg), neg_var)\n",
    "    optimizer.zero_grad()\n",
    "    gradCrit = loss_neg.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return model(paper, p_pos), model(paper,p_neg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch: 0. Time 1.2754223346710205 av_pos: 0.02194072071619303. av_neg: 0.0821487883105874\n",
      "Epoch: 0. Batch: 1. Time 1.3558611869812012 av_pos: 0.022787900837047348. av_neg: 0.08486065577715635\n",
      "Epoch: 0. Batch: 2. Time 1.3750531673431396 av_pos: 0.023493933553824415. av_neg: 0.08452777655795217\n",
      "Epoch: 0. Batch: 3. Time 1.3301098346710205 av_pos: 0.023863562273918434. av_neg: 0.08382976029533892\n",
      "Epoch: 0. Batch: 4. Time 1.31687331199646 av_pos: 0.024379226570919857. av_neg: 0.08241948178783058\n",
      "Epoch: 0. Batch: 5. Time 1.3350005149841309 av_pos: 0.023964836281678194. av_neg: 0.08224251994552711\n",
      "Epoch: 0. Batch: 6. Time 1.3945679664611816 av_pos: 0.02614211866248037. av_neg: 0.0814134466914194\n",
      "Epoch: 0. Batch: 7. Time 1.35892915725708 av_pos: 0.02875351283325017. av_neg: 0.079891101969406\n",
      "Epoch: 0. Batch: 8. Time 1.24531888961792 av_pos: 0.03120878618291802. av_neg: 0.0788178363463117\n",
      "Epoch: 0. av_pos: 0.03120878618291802. av_neg: 0.0788178363463117. AUC: 0.5625\n",
      "Epoch: 1. Batch: 0. Time 1.284379482269287 av_pos: 0.023370063795991883. av_neg: 0.08927197858691216\n",
      "Epoch: 1. Batch: 1. Time 1.3242583274841309 av_pos: 0.023953896874063504. av_neg: 0.08955183437094093\n",
      "Epoch: 1. Batch: 2. Time 1.3382983207702637 av_pos: 0.024720586097234144. av_neg: 0.09302110854536294\n",
      "Epoch: 1. Batch: 3. Time 1.3163025379180908 av_pos: 0.025160626637525642. av_neg: 0.09249027133453637\n",
      "Epoch: 1. Batch: 4. Time 1.258756399154663 av_pos: 0.02574898277092143. av_neg: 0.08955269068852066\n",
      "Epoch: 1. Batch: 5. Time 1.2631394863128662 av_pos: 0.02527758484911222. av_neg: 0.09024860812661549\n",
      "Epoch: 1. Batch: 6. Time 1.3236043453216553 av_pos: 0.02749786439732686. av_neg: 0.08997327044872301\n",
      "Epoch: 1. Batch: 7. Time 1.353318691253662 av_pos: 0.03003706500636781. av_neg: 0.08829765994800255\n",
      "Epoch: 1. Batch: 8. Time 1.3031868934631348 av_pos: 0.03247606632199879. av_neg: 0.08704195264933838\n",
      "Epoch: 1. av_pos: 0.03247606632199879. av_neg: 0.08704195264933838. AUC: 0.5714285714285714\n",
      "Epoch: 2. Batch: 0. Time 1.273698329925537 av_pos: 0.02459453273255349. av_neg: 0.09418652771040797\n",
      "Epoch: 2. Batch: 1. Time 1.288388967514038 av_pos: 0.025215088455788646. av_neg: 0.096361552933231\n",
      "Epoch: 2. Batch: 2. Time 1.2699949741363525 av_pos: 0.02597994918327458. av_neg: 0.09671813374385237\n",
      "Epoch: 2. Batch: 3. Time 1.2960002422332764 av_pos: 0.02646895609724197. av_neg: 0.09648161485325545\n",
      "Epoch: 2. Batch: 4. Time 1.365100383758545 av_pos: 0.027075816259108248. av_neg: 0.09347794170305132\n",
      "Epoch: 2. Batch: 5. Time 1.355292797088623 av_pos: 0.026576928864439343. av_neg: 0.09489980018697679\n",
      "Epoch: 2. Batch: 6. Time 1.3211307525634766 av_pos: 0.02888021033130956. av_neg: 0.09560770701084818\n",
      "Epoch: 2. Batch: 7. Time 1.2069287300109863 av_pos: 0.031319493290761787. av_neg: 0.09345947861904279\n",
      "Epoch: 2. Batch: 8. Time 1.3811469078063965 av_pos: 0.033745995105164184. av_neg: 0.09271099105477333\n",
      "Epoch: 2. av_pos: 0.033745995105164184. av_neg: 0.09271099105477333. AUC: 0.59375\n",
      "Epoch: 3. Batch: 0. Time 1.2641963958740234 av_pos: 0.026137366633447526. av_neg: 0.10455533862113953\n",
      "Epoch: 3. Batch: 1. Time 1.340404748916626 av_pos: 0.026578558667879407. av_neg: 0.10476998098194598\n",
      "Epoch: 3. Batch: 2. Time 1.3712925910949707 av_pos: 0.02736322221223721. av_neg: 0.10475722458213567\n",
      "Epoch: 3. Batch: 3. Time 1.3107151985168457 av_pos: 0.02784202870695481. av_neg: 0.10510038173291832\n",
      "Epoch: 3. Batch: 4. Time 1.3306033611297607 av_pos: 0.028546533442189687. av_neg: 0.10343986455723644\n",
      "Epoch: 3. Batch: 5. Time 1.288339614868164 av_pos: 0.02806797805059129. av_neg: 0.10344901217768591\n",
      "Epoch: 3. Batch: 6. Time 1.3057587146759033 av_pos: 0.03041461601003643. av_neg: 0.10319680226700648\n",
      "Epoch: 3. Batch: 7. Time 1.262373447418213 av_pos: 0.03279941329183657. av_neg: 0.10165555665036663\n",
      "Epoch: 3. Batch: 8. Time 1.2868266105651855 av_pos: 0.03515110270046231. av_neg: 0.10038462661560416\n",
      "Epoch: 3. av_pos: 0.03515110270046231. av_neg: 0.10038462661560416. AUC: 0.5982142857142857\n",
      "Epoch: 4. Batch: 0. Time 1.298220157623291 av_pos: 0.02744001026654587. av_neg: 0.10351501252502203\n",
      "Epoch: 4. Batch: 1. Time 1.3134970664978027 av_pos: 0.027869679788036592. av_neg: 0.10817772803828121\n",
      "Epoch: 4. Batch: 2. Time 1.2736077308654785 av_pos: 0.028739919519806183. av_neg: 0.11080359801650048\n",
      "Epoch: 4. Batch: 3. Time 1.2911901473999023 av_pos: 0.029343729686670483. av_neg: 0.11060428375378252\n",
      "Epoch: 4. Batch: 4. Time 1.3350586891174316 av_pos: 0.0301168517563201. av_neg: 0.11001655808091164\n",
      "Epoch: 4. Batch: 5. Time 1.3915228843688965 av_pos: 0.029614259568152192. av_neg: 0.11101632748420041\n",
      "Epoch: 4. Batch: 6. Time 1.2845501899719238 av_pos: 0.03199654266702314. av_neg: 0.10973158313227552\n",
      "Epoch: 4. Batch: 7. Time 1.2512218952178955 av_pos: 0.034282986011717184. av_neg: 0.10693236091407016\n",
      "Epoch: 4. Batch: 8. Time 1.3092775344848633 av_pos: 0.036603664015844216. av_neg: 0.10571208494818873\n",
      "Epoch: 4. av_pos: 0.036603664015844216. av_neg: 0.10571208494818873. AUC: 0.5982142857142857\n",
      "Epoch: 5. Batch: 0. Time 1.272742748260498 av_pos: 0.02869555856257648. av_neg: 0.11661648742854595\n",
      "Epoch: 5. Batch: 1. Time 1.3133971691131592 av_pos: 0.029108482570063644. av_neg: 0.11877828726544976\n",
      "Epoch: 5. Batch: 2. Time 1.2108643054962158 av_pos: 0.030069500867721217. av_neg: 0.1195984415213267\n",
      "Epoch: 5. Batch: 3. Time 1.3590726852416992 av_pos: 0.030764192751239535. av_neg: 0.11877047849819064\n",
      "Epoch: 5. Batch: 4. Time 1.3620026111602783 av_pos: 0.0316233554451901. av_neg: 0.11555869624018669\n",
      "Epoch: 5. Batch: 5. Time 1.2844741344451904 av_pos: 0.031059161047075273. av_neg: 0.11661537737275163\n",
      "Epoch: 5. Batch: 6. Time 1.391101360321045 av_pos: 0.03352469261601072. av_neg: 0.117509299681655\n",
      "Epoch: 5. Batch: 7. Time 1.2889230251312256 av_pos: 0.0357880962022. av_neg: 0.11604387536644936\n",
      "Epoch: 5. Batch: 8. Time 1.3737602233886719 av_pos: 0.038109808548571086. av_neg: 0.11509503449416823\n",
      "Epoch: 5. av_pos: 0.038109808548571086. av_neg: 0.11509503449416823. AUC: 0.6071428571428571\n",
      "Epoch: 6. Batch: 0. Time 1.2566125392913818 av_pos: 0.030357240008706866. av_neg: 0.12417919717729092\n",
      "Epoch: 6. Batch: 1. Time 1.2718281745910645 av_pos: 0.030805269023894653. av_neg: 0.12825541844591498\n",
      "Epoch: 6. Batch: 2. Time 1.306931495666504 av_pos: 0.031709926977770614. av_neg: 0.12595360136720654\n",
      "Epoch: 6. Batch: 3. Time 1.3713276386260986 av_pos: 0.0324815208002201. av_neg: 0.12714964238325593\n",
      "Epoch: 6. Batch: 4. Time 1.3208253383636475 av_pos: 0.033444357952073914. av_neg: 0.12486221382286022\n",
      "Epoch: 6. Batch: 5. Time 1.3213591575622559 av_pos: 0.0328563437148235. av_neg: 0.12518650571104448\n",
      "Epoch: 6. Batch: 6. Time 1.242805004119873 av_pos: 0.03537168607064294. av_neg: 0.12491026240988766\n",
      "Epoch: 6. Batch: 7. Time 1.3345777988433838 av_pos: 0.03759103041801154. av_neg: 0.12335170522031605\n",
      "Epoch: 6. Batch: 8. Time 1.352952241897583 av_pos: 0.03991095251109881. av_neg: 0.1219012179364533\n",
      "Epoch: 6. av_pos: 0.03991095251109881. av_neg: 0.1219012179364533. AUC: 0.6160714285714286\n",
      "Epoch: 7. Batch: 0. Time 1.2053394317626953 av_pos: 0.03217429160296888. av_neg: 0.130185970030725\n",
      "Epoch: 7. Batch: 1. Time 1.2280972003936768 av_pos: 0.03241320546794668. av_neg: 0.13273536885157228\n",
      "Epoch: 7. Batch: 2. Time 1.3333396911621094 av_pos: 0.03336646094543539. av_neg: 0.13648511651903392\n",
      "Epoch: 7. Batch: 3. Time 1.3382480144500732 av_pos: 0.03412716009072937. av_neg: 0.13465701568871735\n",
      "Epoch: 7. Batch: 4. Time 1.340092658996582 av_pos: 0.03516510586554068. av_neg: 0.1333701361566782\n",
      "Epoch: 7. Batch: 5. Time 1.2686891555786133 av_pos: 0.03454815060512677. av_neg: 0.132877331332614\n",
      "Epoch: 7. Batch: 6. Time 1.3440861701965332 av_pos: 0.037115179424274954. av_neg: 0.13430248531379868\n",
      "Epoch: 7. Batch: 7. Time 1.319777011871338 av_pos: 0.03931448934698437. av_neg: 0.1324216003390029\n",
      "Epoch: 7. Batch: 8. Time 1.3389105796813965 av_pos: 0.04157756536251933. av_neg: 0.12983201783564355\n",
      "Epoch: 7. av_pos: 0.04157756536251933. av_neg: 0.12983201783564355. AUC: 0.5803571428571429\n",
      "Epoch: 8. Batch: 0. Time 1.3261802196502686 av_pos: 0.03376613809417904. av_neg: 0.13683189865201711\n",
      "Epoch: 8. Batch: 1. Time 1.4124562740325928 av_pos: 0.03406343602464403. av_neg: 0.1453045452106744\n",
      "Epoch: 8. Batch: 2. Time 1.3184571266174316 av_pos: 0.03505113355625023. av_neg: 0.14386804109439252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8. Batch: 3. Time 1.3771741390228271 av_pos: 0.03588277766038118. av_neg: 0.14243358528707176\n",
      "Epoch: 8. Batch: 4. Time 1.35357666015625 av_pos: 0.036961037310567914. av_neg: 0.14002531630918383\n",
      "Epoch: 8. Batch: 5. Time 1.332549810409546 av_pos: 0.03630315011014317. av_neg: 0.14107720802758195\n",
      "Epoch: 8. Batch: 6. Time 1.339008092880249 av_pos: 0.03891681116226599. av_neg: 0.1396719622710693\n",
      "Epoch: 8. Batch: 7. Time 1.3642454147338867 av_pos: 0.04102227429488721. av_neg: 0.13757943477646678\n",
      "Epoch: 8. Batch: 8. Time 1.3498926162719727 av_pos: 0.04327309034331847. av_neg: 0.13574253937583208\n",
      "Epoch: 8. av_pos: 0.04327309034331847. av_neg: 0.13574253937583208. AUC: 0.5892857142857143\n",
      "Epoch: 9. Batch: 0. Time 1.289754867553711 av_pos: 0.03535583787928772. av_neg: 0.14618756148964165\n",
      "Epoch: 9. Batch: 1. Time 1.2452471256256104 av_pos: 0.035646155736958465. av_neg: 0.1534706825762987\n",
      "Epoch: 9. Batch: 2. Time 1.3615360260009766 av_pos: 0.036678173833194404. av_neg: 0.15263691001882154\n",
      "Epoch: 9. Batch: 3. Time 1.23948335647583 av_pos: 0.03749901411750216. av_neg: 0.15039738562889396\n",
      "Epoch: 9. Batch: 4. Time 1.2768309116363525 av_pos: 0.038609776468802016. av_neg: 0.14715768124163148\n",
      "Epoch: 9. Batch: 5. Time 1.3210699558258057 av_pos: 0.03791837824975422. av_neg: 0.14842549154534934\n",
      "Epoch: 9. Batch: 6. Time 1.3654398918151855 av_pos: 0.04055819658103636. av_neg: 0.14747868954337068\n",
      "Epoch: 9. Batch: 7. Time 1.300145149230957 av_pos: 0.04264842482185941. av_neg: 0.146018801855389\n",
      "Epoch: 9. Batch: 8. Time 1.328676700592041 av_pos: 0.04483621871412955. av_neg: 0.14499445179477333\n",
      "Epoch: 9. av_pos: 0.04483621871412955. av_neg: 0.14499445179477333. AUC: 0.6339285714285714\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    aver_pos = list()\n",
    "    aver_neg = list()\n",
    "    \n",
    "    batch=0\n",
    "    t0=0\n",
    "    t1=0\n",
    "    triples_iterator = prepare_triples(_training_set[:3000])\n",
    "    \n",
    "    for arg1,arg2,arg3 in triples_iterator:\n",
    "        t0 = time.time()\n",
    "#         print('sampling time: {0}'.format(t0-t1))\n",
    "        ap, an = train_networks(model=dist_model, criteria=crit, optimizer=optim, \n",
    "                                paper=make_var(arg1), p_pos=make_var(arg2), \n",
    "                                p_neg=make_var(arg3))\n",
    "        t1 = time.time()\n",
    "        total= t1-t0\n",
    "        aver_pos.append(ap.data.mean())\n",
    "        aver_neg.append(an.data.mean())\n",
    "        print('Epoch: {0}. Batch: {3}. Time {4} av_pos: {1}. av_neg: {2}'.format(i, np.mean(aver_pos), \n",
    "                                                                                 np.mean(aver_neg), \n",
    "                                                                                 batch, total))\n",
    "        batch+=1\n",
    "    \n",
    "    #calc auc\n",
    "    auc = calculate_AUC(model=dist_model, val_set=_training_set[3000:4000])\n",
    "    \n",
    "    print('Epoch: {0}. av_pos: {1}. av_neg: {2}. AUC: {3}'.format(i, np.mean(aver_pos), np.mean(aver_neg), auc))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triple_iterator1 = get_triple(training_set)\n",
    "batch_iterator1 = get_batch(triple_iterator1)\n",
    "arg1,arg2,arg3 = next(batch_iterator1)\n",
    "k = dist_model(arg1,arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RECALL_N = 10\n",
    "\n",
    "def calculate_AUC(model, val_set):\n",
    "    neg_examples_amount = min(100, int(len(val_set)/10))\n",
    "    a=0\n",
    "    recall_at_n = list()\n",
    "    \n",
    "    for paper_doi in val_set:\n",
    "        paper = _paper_dict[paper_doi]\n",
    "        pos_ref_set = get_papers_per_authors(paper).intersection(val_set)\n",
    "        pos_ref_set.update(set(_ref_dict[paper_doi]).intersection(val_set))\n",
    "        if len(pos_ref_set)==0:\n",
    "            continue\n",
    "        \n",
    "        neg_ref_set = set()\n",
    "        for n in range(neg_examples_amount):\n",
    "            new_id = None\n",
    "            while((new_id in pos_ref_set) or new_id==None):\n",
    "                new_id = val_set[random.randrange(0,neg_examples_amount)]\n",
    "            neg_ref_set.add(new_id)\n",
    "        \n",
    "        all_refs = set()\n",
    "        all_refs.update(pos_ref_set)\n",
    "        all_refs.update(neg_ref_set)\n",
    "        \n",
    "        paper_vector = model.calculate_res(make_var(paper.abstract))\n",
    "        \n",
    "        #calculate\n",
    "        distance_array = dict()\n",
    "        for ref in all_refs: \n",
    "            ref_vec = model.calculate_res(make_var(_paper_dict[ref].abstract))\n",
    "            ref_dist = np.linalg.norm(paper_vector - ref_vec)\n",
    "            distance_array[ref] = ref_dist\n",
    "\n",
    "        ordered_predicted_list = list(map(lambda x: x[0], \n",
    "                                          sorted(distance_array.items(), \n",
    "                                                 key=lambda val: val[1])))[:RECALL_N]\n",
    "        recall = len(pos_ref_set.intersection(ordered_predicted_list))/len(pos_ref_set)\n",
    "        if a!=0:\n",
    "            print(ordered_predicted_list)\n",
    "            print(pos_ref_set)\n",
    "            a-=1\n",
    "        #print(recall)\n",
    "        recall_at_n.append(recall)\n",
    "\n",
    "    return np.mean(recall_at_n)\n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':1,'b':5, 'c':3,'d':0}\n",
    "b = list(map(lambda x: x[0], sorted(a.items(), key=lambda val: val[1])[:3]))\n",
    "c = {'d', 'a'}\n",
    "d = len(c.intersection(b))/len(c)\n",
    "np.mean([d,0])\n",
    "int(10 / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
