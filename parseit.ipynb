{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import parser\n",
    "import numpy as np\n",
    "import gzip, msgpack\n",
    "from torch.nn.modules import PairwiseDistance\n",
    "import random\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import scipy.sparse as sp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "N_FEATURES = 2*18\n",
    "hv = HashingVectorizer(n_features=N_FEATURES,non_negative=True, analyzer='word',\n",
    "                       ngram_range=(3,3), norm='l2', stop_words='english')\n",
    "# try default n-gram 1-gram,2-gram,3-gram\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "def convert_to_variable(abstract):\n",
    "#     pos_x = abstract.nonzero()[0].tolist()\n",
    "#     pos_y = abstract.nonzero()[1].tolist()\n",
    "\n",
    "#     i = torch.LongTensor([pos_x, pos_y])\n",
    "#     v = torch.FloatTensor([a[1].abstract[x,y] for x,y in zip(pos_x,pos_y)])\n",
    "#     sparse = torch.sparse.FloatTensor(i, v, torch.Size([1,N_FEATURES]))\n",
    "    sparse = torch.from_numpy(abstract.todense()).type(dtype)\n",
    "    tensor = Variable(sparse)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class SmallReference:\n",
    "    def __init__(self, identifiers):\n",
    "        self.identifiers = identifiers\n",
    "\n",
    "class SmallPaper:\n",
    "    def __init__(self, title, identifiers, authors, abstract, refs):\n",
    "        self.title = title\n",
    "        self.authors = authors\n",
    "#         transformed = \n",
    "        self.abstract = hv.transform([abstract])\n",
    "        self.references = [SmallReference(ref.identifiers) for ref in refs]\n",
    "        self.identifiers = identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doi(paper):\n",
    "    _ref_dict = dict()\n",
    "    for x in paper.identifiers:\n",
    "#        @todo lowecase\n",
    "        _ref_dict[x.key_type] = x.key.lower()\n",
    "    if 'doi' in _ref_dict:\n",
    "        return _ref_dict['doi']\n",
    "    elif 'ieee' in _ref_dict:\n",
    "        return _ref_dict['ieee']\n",
    "    elif 'semanticscholar' in _ref_dict:\n",
    "        return _ref_dict['semanticscholar']\n",
    "    elif 'adsa' in _ref_dict:\n",
    "        return _ref_dict['adsa']\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def update_authors(ident_to_auth, authors_to_papers, paper):\n",
    "    for author in paper.authors:\n",
    "        author_name = author.name.name\n",
    "        \n",
    "        if not author_name:\n",
    "            continue\n",
    "        for identifier in author.identifiers:\n",
    "            result = ident_to_auth.get(identifier.key.lower(), False) \n",
    "            if result:\n",
    "                author_name = str(result)\n",
    "                break\n",
    "        for identifier in author.identifiers:\n",
    "            ident_to_auth[identifier.key.lower()] = author_name\n",
    "        if not authors_to_papers.get(author_name, False):\n",
    "            authors_to_papers[author_name] = list()\n",
    "        authors_to_papers[author_name].append(get_doi(paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:adsa:2000JMatS..35.4393W title is empty\n",
      "WARNING:root:adsa:2002RuPhJ..45..389B title is empty\n",
      "WARNING:root:doi:10.21609/jiki.v4i1.155 title is empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total count: 43293\n",
      "{'acm': 5768, 'doi': 22937, 'springer': 2507, 'onepetro': 122, 'adsa': 6875, 'oxford': 697, 'science': 14, 'ieee': 10322, 'nature': 348, 'semanticscholar': 8002, 'iop': 530, 'pmid': 3932, 'mid': 66, 'pmc': 1132, 'sage': 133, 'pmcid': 1, 'arxiv': 1543, 'manuscript': 124}\n"
     ]
    }
   ],
   "source": [
    "MAX_ELEMENTS = 30000\n",
    "train_test_val_share = [0.5*MAX_ELEMENTS, 0.4*MAX_ELEMENTS, MAX_ELEMENTS]\n",
    "\n",
    "identies_set = dict()\n",
    "\n",
    "_paper_dict = dict()\n",
    "myiter = 0\n",
    "count=0\n",
    "_ref_dict = dict()\n",
    "_training_set = list()\n",
    "_validation_set = list()\n",
    "_test_set = list()\n",
    "_ident_to_auth_dict = dict()\n",
    "_authors_to_papers_dict = dict()\n",
    "with gzip.open(\"NN_Papers.msgpack.gz\", \"rb\") as nn_papers_out:\n",
    "    unpacker = msgpack.Unpacker(nn_papers_out, encoding='utf-8')\n",
    "    for _paper in unpacker:\n",
    "        count+=1\n",
    "        paper = parser.Paper.deserialize(_paper)\n",
    "        \n",
    "        paper_doi = get_doi(paper)\n",
    "        if not (paper_doi and paper.abstract and paper.abstract.strip()):\n",
    "            continue\n",
    "        else:\n",
    "            myiter+=1\n",
    "        paper = SmallPaper(paper.title, paper.identifiers, paper.authors, paper.abstract.strip(), paper.references)\n",
    "        if myiter>=MAX_ELEMENTS:\n",
    "            break\n",
    "        #calculate statistics\n",
    "        for iden in paper.identifiers:\n",
    "            if identies_set.get(iden.key_type, False):\n",
    "                identies_set[iden.key_type]+=1\n",
    "            else:\n",
    "                identies_set[iden.key_type]=1\n",
    "        #end\n",
    "        \n",
    "        _ref_dict[paper_doi] = list()\n",
    " \n",
    "        update_authors(_ident_to_auth_dict, _authors_to_papers_dict, paper)\n",
    "        _paper_dict[paper_doi] = paper\n",
    "        if myiter<=train_test_val_share[0]:\n",
    "            _training_set.append(paper_doi)\n",
    "        elif myiter<=train_test_val_share[1]: \n",
    "            _test_set.append(paper_doi)\n",
    "        else:\n",
    "            _validation_set.append(paper_doi)\n",
    "\n",
    "print('total count: {0}'.format(count))\n",
    "print(identies_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8218\n"
     ]
    }
   ],
   "source": [
    "def update_references(paper_dict, ref_dict):\n",
    "    count = 0\n",
    "    for paper_doi,paper in paper_dict.items():\n",
    "        for ref in paper.references:\n",
    "            ref_doi = get_doi(ref)\n",
    "            if not ref_doi:\n",
    "                continue\n",
    "            result = ref_dict.get(ref_doi, False)\n",
    "            if result!=False:\n",
    "                count+=1\n",
    "                ref_dict[paper_doi].append(ref_doi)\n",
    "    print(count)\n",
    "update_references(_paper_dict, _ref_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_var(abstract):\n",
    "    dense = abstract.todense()\n",
    "    torch_tensor = Variable(torch.from_numpy(dense).type(dtype))\n",
    "    return torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# appr = _paper_dict['10.1109/tnnls.2015.2392563']\n",
    "def get_papers_per_authors(paper):\n",
    "    set_to_merge = set()\n",
    "    for author in paper.authors:\n",
    "        identifier = list(author.identifiers)[0].key\n",
    "#         print(\"name {0}. papers: {1}\".format(_ident_to_auth_dict[identifier],\n",
    "#                                              _authors_to_papers_dict[_ident_to_auth_dict[identifier]]))\n",
    "        set_to_merge.update(set(_authors_to_papers_dict[_ident_to_auth_dict[identifier.lower()]]))\n",
    "    set_to_merge.discard(get_doi(paper))\n",
    "    return set_to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "\n",
    "def prepare_triples(val_set):\n",
    "    count = 0\n",
    "    result_list = list()\n",
    "    batch_list_1 = None\n",
    "    batch_list_2 = None\n",
    "    batch_list_3 = None\n",
    "#     print(len(val_set))\n",
    "    for paper_doi in val_set:\n",
    "        paper = _paper_dict[paper_doi]\n",
    "        \n",
    "        \n",
    "        pos_ref_set = get_papers_per_authors(paper)\n",
    "        \n",
    "        pos_ref_set.update(_ref_dict[paper_doi])\n",
    "        pos_ref_set = (pos_ref_set.difference(_validation_set)).difference(_test_set)\n",
    "\n",
    "        #cut down articles that are not used in val_set\n",
    "        \n",
    "#         pos_ref_set.intersection(val_set)\n",
    "        \n",
    "        if len(pos_ref_set) < 4:\n",
    "#             print(paper_doi)\n",
    "            continue\n",
    "#         print(len(pos_ref_set))\n",
    "        neg_ref_set = set()\n",
    "        for n in range(len(pos_ref_set)):\n",
    "            new_id = None\n",
    "            while((new_id in pos_ref_set) or new_id==None):\n",
    "                new_id = val_set[random.randrange(0,len(val_set)-1)]\n",
    "            neg_ref_set.add(new_id)\n",
    "            \n",
    "            \n",
    "        for pos,neg in zip(pos_ref_set, neg_ref_set):\n",
    "\n",
    "            if batch_list_1 == None:\n",
    "                batch_list_1 = paper.abstract\n",
    "                batch_list_2 = _paper_dict[pos].abstract\n",
    "                batch_list_3 = _paper_dict[neg].abstract\n",
    "            else:\n",
    "                batch_list_1 = sp.vstack((batch_list_1, paper.abstract))\n",
    "                batch_list_2 = sp.vstack((batch_list_2, _paper_dict[pos].abstract))\n",
    "                batch_list_3 = sp.vstack((batch_list_3, _paper_dict[neg].abstract))\n",
    "\n",
    "            if batch_list_1.shape[0]==BATCH_SIZE:\n",
    "                yield (make_var(batch_list_1), make_var(batch_list_2), make_var(batch_list_3))\n",
    "                batch_list_1 = None\n",
    "#     print(count)\n",
    "    return result_list\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistRes(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistRes, self).__init__()\n",
    "        self.layer1 = nn.Linear(N_FEATURES, 50)\n",
    "        self.layer2 = nn.PairwiseDistance(2)\n",
    "\n",
    "    def forward(self, x1,x2):\n",
    "#         print('typex: {0}'.format(type(x1)))\n",
    "#         print('typey: {0}'.format(type(x2)))\n",
    "        res1 = torch.nn.functional.tanh(self.layer1(x1))\n",
    "        res2 = torch.nn.functional.tanh(self.layer1(x2))\n",
    "\n",
    "#         res3 = self.layer1(x3)\n",
    "        # normalize vector L2-normalization\n",
    "        return torch.nn.functional.tanh(self.layer2(res1,res2))\n",
    "    \n",
    "    def calculate_res(self, x):\n",
    "        return torch.nn.functional.tanh(self.layer1(x)).data.numpy()\n",
    "    \n",
    "# linear/RelU/linear\n",
    "# dropout input layer\n",
    "# help as they authored by same person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dist_model = DistRes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check the distance Loss, just to see the difference\n",
    "# maybe pos and pos close\n",
    "\n",
    "crit = nn.HingeEmbeddingLoss(1)\n",
    "optim = torch.optim.RMSprop(dist_model.parameters(), momentum=0.5, lr=0.01)\n",
    "# local_dist(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones_var = Variable(torch.ones(BATCH_SIZE))\n",
    "neg_var = Variable(torch.ones(BATCH_SIZE)*(-1))\n",
    "\n",
    "def train_networks(model, criteria, optimizer, paper, p_pos, p_neg):\n",
    "\n",
    "    loss_pos = criteria.forward( model(paper, p_pos), ones_var)\n",
    "    optimizer.zero_grad()\n",
    "    gradCrit = loss_pos.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_neg = criteria.forward( model(paper,p_neg), neg_var)\n",
    "    optimizer.zero_grad()\n",
    "    gradCrit = loss_neg.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return model(paper, p_pos), model(paper,p_neg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RECALL_N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_for_recall(val_set, items_to_validate=100, neg_examples_amount=200):\n",
    "\n",
    "    recall_list = list()\n",
    "    \n",
    "    total_ref_dict = dict()\n",
    "    rand_res_list = list()\n",
    "    paper_index = 0\n",
    "    recall_iter = items_to_validate\n",
    "    while(recall_iter!=0):\n",
    "        paper_doi = valid_set[paper_index]\n",
    "        paper_index +=1\n",
    "        paper = _paper_dict[paper_doi]\n",
    "        pos_ref_set = get_papers_per_authors(paper)\n",
    "        pos_ref_set.update(_ref_dict[paper_doi])\n",
    "\n",
    "        pos_ref_set.intersection(val_set)\n",
    "\n",
    "        \n",
    "        if len(pos_ref_set)<4:\n",
    "            continue\n",
    "        recall_iter -= 1\n",
    "\n",
    "        neg_examples_amount = 200\n",
    "        neg_ref_set = set()\n",
    "        for n in range(neg_examples_amount):\n",
    "            new_id = None\n",
    "            while((new_id in pos_ref_set) or new_id==None or (new_id in neg_ref_set)):\n",
    "                new_id = train_set[random.randrange(0,len(train_set))]\n",
    "            neg_ref_set.add(new_id)\n",
    "        \n",
    "        all_refs = set()\n",
    "        all_refs.update(pos_ref_set)\n",
    "        all_refs.update(neg_ref_set)\n",
    "        rand_res_list.append((len(pos_ref_set)/len(all_refs))*RECALL_N/len(pos_ref_set))\n",
    "        \n",
    "        for ref_doi in all_refs:\n",
    "            total_ref_dict[ref_doi] = make_var(_paper_dict[ref_doi].abstract)\n",
    "        total_ref_dict[paper_doi] = make_var(_paper_dict[paper_doi].abstract)\n",
    "        \n",
    "        recall_list.append((paper_doi, \n",
    "                            pos_ref_set, \n",
    "                            all_refs))\n",
    "\n",
    " \n",
    "    rand_res = 'random result: {0}'.format(np.mean(rand_res_list))\n",
    "\n",
    "    return recall_list, total_ref_dict, rand_res\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_recall(model, recall_list, total_ref_dict):\n",
    "\n",
    "    a=0\n",
    "    recall_at_n = list()\n",
    "    \n",
    "    #calculate all vectors for the set\n",
    "    doi_to_vecs = dict()\n",
    "    count_1 = 0\n",
    "#     t0 = time.time()\n",
    "    for key,value in total_ref_dict.items():\n",
    "        doi_to_vecs[key] = model.calculate_res(value)\n",
    "#     print(time.time()-t0)\n",
    "    \n",
    "#     t0 = time.time()\n",
    "    \n",
    "    for paper_doi, pos_ref_set, all_refs in recall_list:\n",
    "\n",
    "        paper_vector = doi_to_vecs[paper_doi]\n",
    "\n",
    "        distance_array = dict()\n",
    "        for ref_doi in all_refs: \n",
    "            ref_vec = doi_to_vecs[ref_doi]\n",
    "            ref_dist = np.linalg.norm(paper_vector - ref_vec)\n",
    "            distance_array[ref_doi] = ref_dist\n",
    "\n",
    "        ordered_predicted_list = list(map(lambda x: x[0], \n",
    "                                          sorted(distance_array.items(), \n",
    "                                                 key=lambda val: val[1])))[:RECALL_N]\n",
    "        recall = len(pos_ref_set.intersection(ordered_predicted_list)) / len(pos_ref_set)\n",
    "        if a!=0 and len(pos_ref_set)!=1:\n",
    "            print('recall: {2}. expected: {0}, got {1}'.format(pos_ref_set, ordered_predicted_list, recall))\n",
    "\n",
    "            a-=1\n",
    "        #print(recall)\n",
    "        recall_at_n.append(recall)\n",
    "#     print(count_1)\n",
    "#     print(time.time()-t0)\n",
    "    return np.mean(recall_at_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = _training_set\n",
    "valid_set = _validation_set\n",
    "# t0=time.time()\n",
    "\n",
    "# print(time.time()-t0)\n",
    "# print(len(triples_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_time = time.time()\n",
    "\n",
    "def write_to_file(file, text_to_write):\n",
    "    print(text_to_write)\n",
    "    file.write(text_to_write)\n",
    "    file.write('\\n')\n",
    "    file.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random result: 0.02392248943273766\n",
      "Beginning recall: 0.02227366152366152\n",
      "Epoch: 0. Batch: 100. Time 9.827518701553345 av_pos: 0.9458517710575945. av_neg: 0.9572097988884047\n",
      "Epoch: 0. Batch: 200. Time 10.235700845718384 av_pos: 0.9563150741658036. av_neg: 0.9664805687660009\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "epoch_auc = dict()\n",
    "my_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "f = open('/home/jupyter/output_{0}.txt'.format(my_time),'w')\n",
    "\n",
    "recall_list, recall_ref_dict, text = validation_for_recall(train_set)\n",
    "write_to_file(f, text)\n",
    "\n",
    "recall = calculate_recall(dist_model, recall_list, recall_ref_dict)\n",
    "text = 'Beginning recall: {0}'.format(recall)\n",
    "write_to_file(f, text)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    aver_pos = list()\n",
    "    aver_neg = list()\n",
    "    triples_iterator = prepare_triples(train_set)    \n",
    "    batch=0\n",
    "    t0 = time.time()\n",
    "    for arg1,arg2,arg3 in triples_iterator:\n",
    "\n",
    "        ap, an = train_networks(model=dist_model, criteria=crit, optimizer=optim, \n",
    "                                paper=arg1, p_pos=arg2, \n",
    "                                p_neg=arg3)\n",
    "        \n",
    "        aver_pos.append(ap.data.mean())\n",
    "        aver_neg.append(an.data.mean())\n",
    "        if (batch % 100 == 0 and batch != 0 ):\n",
    "            t1 = time.time()\n",
    "            total= t1-t0\n",
    "            text = 'Epoch: {0}. Batch: {3}. Time {4} av_pos: {1}. av_neg: {2}'.format(epoch, np.mean(aver_pos), \n",
    "                                                                               np.mean(aver_neg), \n",
    "                                                                                 batch, total)\n",
    "            write_to_file(f, text)\n",
    "            t0 = time.time()\n",
    "        batch+=1\n",
    "    \n",
    "    #calc auc\n",
    "    t0 = time.time()\n",
    "    recall = calculate_recall(dist_model, recall_list, recall_ref_dict)\n",
    "    t1 = time.time()\n",
    "#     print('recall calculated in {0} s.'.format(t1-t0))\n",
    "    epoch_auc[epoch] = recall\n",
    "    text = 'Epoch: {0}. av_pos: {1}. av_neg: {2}. recall: {3}'.format(epoch, np.mean(aver_pos), np.mean(aver_neg), recall)\n",
    "    write_to_file(f, text)\n",
    "\n",
    "f.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_list, recall_ref_dict, text = validation_for_recall(train_set)\n",
    "print(text)\n",
    "text = calculate_recall(dist_model, recall_list, recall_ref_dict)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = {'a':1,'b':5, 'c':3,'d':0}\n",
    "b = list(map(lambda x: x[0], sorted(a.items(), key=lambda val: val[1])[:3]))\n",
    "c = {'d', 'a'}\n",
    "d = len(c.intersection(b))/len(c)\n",
    "np.mean([d,0])\n",
    "int(10 / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
