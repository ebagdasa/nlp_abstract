{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_idents(paper):\n",
    "#     xid = dict()\n",
    "#     for x in paper.identifiers:\n",
    "#         xid[x.key_type] = x.key\n",
    "#     if paper.title:\n",
    "#         xid['title'] = paper.title\n",
    "#     return xid\n",
    "\n",
    "# def compare_idents(pap1, pap2):\n",
    "#     for key, value in pap1.items():\n",
    "#         if pap2.get(key, False) == value:\n",
    "#             return True\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:adsa:2000JMatS..35.4393W title is empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total count: 28811\n",
      "{'nature': 241, 'semanticscholar': 5603, 'ieee': 6969, 'arxiv': 1016, 'pmcid': 1, 'oxford': 718, 'onepetro': 130, 'sage': 86, 'mid': 45, 'science': 10, 'doi': 22034, 'pmc': 792, 'adsa': 4926, 'iop': 368, 'springer': 1731, 'pmid': 3080, 'manuscript': 87, 'acm': 5386}\n"
     ]
    }
   ],
   "source": [
    "import gzip, msgpack\n",
    "\n",
    "\n",
    "\n",
    "def get_doi(paper):\n",
    "    _ref_dict = dict()\n",
    "    for x in paper.identifiers:\n",
    "        _ref_dict[x.key_type] = x.key\n",
    "    if 'doi' in _ref_dict:\n",
    "        return _ref_dict['doi']\n",
    "    elif 'ieee' in _ref_dict:\n",
    "        return _ref_dict['ieee']\n",
    "    elif 'semanticscholar' in _ref_dict:\n",
    "        return _ref_dict['semanticscholar']\n",
    "    elif 'adsa' in _ref_dict:\n",
    "        return _ref_dict['adsa']\n",
    "    else:\n",
    "        return False\n",
    "#     _ref_doi = [x.key for x in paper.identifiers if x.key_type=='doi']\n",
    "#     if len(_ref_doi):\n",
    "#         return _ref_doi[0]\n",
    "#     else:\n",
    "#         return False\n",
    "\n",
    "identies_set = dict()\n",
    "\n",
    "paper_list = dict()\n",
    "myiter = 0\n",
    "count=0\n",
    "ref_dict = dict()\n",
    "training_set = list()\n",
    "test_set = list()\n",
    "with gzip.open(\"NN_Papers.msgpack.gz\", \"rb\") as nn_papers_out:\n",
    "    unpacker = msgpack.Unpacker(nn_papers_out, encoding='utf-8')\n",
    "    for _paper in unpacker:\n",
    "        count+=1\n",
    "        for iden in paper.identifiers:\n",
    "            if identies_set.get(iden.key_type, False):\n",
    "                identies_set[iden.key_type]+=1\n",
    "            else:\n",
    "                identies_set[iden.key_type]=1\n",
    "        paper = parser.Paper.deserialize(_paper)\n",
    "        \n",
    "        paper_doi = get_doi(paper)\n",
    "        if not (paper_doi and paper.abstract):\n",
    "            continue\n",
    "        if myiter >=20000: #and ref_dict.get(paper_doi, False):\n",
    "            break\n",
    "        else:\n",
    "            myiter+=1\n",
    "        \n",
    "#         if myiter >=20000: #and ref_dict.get(paper_doi, False):\n",
    "#             continue\n",
    "#         else:\n",
    "#             myiter+=1\n",
    "        # _paper is a python dict object. you can use it directly if you don't want to use the Paper class provided here\n",
    "\n",
    "        ref_dict[paper_doi] = list()\n",
    "        for ref in paper.references:\n",
    "            ref_doi = get_doi(ref)\n",
    "            if not ref_doi:\n",
    "                continue\n",
    "            \n",
    "            if ref_dict.get(ref_doi, False):\n",
    "                ref_dict[ref_doi].append(paper_doi)                \n",
    "            else:\n",
    "                ref_dict[ref_doi] = [paper_doi]\n",
    "\n",
    "            \n",
    "            ref_dict[paper_doi].append(ref_doi)\n",
    "            \n",
    "        \n",
    "        paper_list[paper_doi] = paper\n",
    "        if myiter<=16000:\n",
    "            training_set.append(paper)\n",
    "        else: \n",
    "            test_set.append(paper)\n",
    "\n",
    "print('total count: {0}'.format(count))\n",
    "print(identies_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "N_FEATURES = 2**18\n",
    "hv = HashingVectorizer(n_features=N_FEATURES,non_negative=True, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract_training_set = [paper.abstract for paper in training_set]\n",
    "abstract_training_set_hashed = hv.transform(abstract_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules import PairwiseDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'springer': 1674, 'semanticscholar': 5369, 'ieee': 6924, 'science': 10, 'arxiv': 1016, 'pmcid': 1, 'oxford': 455, 'onepetro': 85, 'sage': 86, 'mid': 43, 'nature': 233, 'doi': 15280, 'pmc': 746, 'adsa': 4542, 'iop': 367, 'acm': 3823, 'pmid': 2607, 'manuscript': 87}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identies_set = dict()\n",
    "for paper in paper_list.values():\n",
    "    for iden in paper.identifiers:\n",
    "        if identies_set.get(iden.key_type, False):\n",
    "            identies_set[iden.key_type]+=1\n",
    "        else:\n",
    "            identies_set[iden.key_type]=1\n",
    "print(identies_set)\n",
    "len(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_triple(val_set):\n",
    "    count = 0\n",
    "    for item in val_set:\n",
    "        count += 1\n",
    "        print('count: {0}'.format(count))\n",
    "        _ref_doi = get_doi(item)\n",
    "        if _ref_doi not in ref_dict.keys():\n",
    "            continue\n",
    "        _ref_list = ref_dict[_ref_doi]\n",
    "        print(_ref_list)\n",
    "        for _ref in _ref_list:\n",
    "            \n",
    "            if paper_list.get(_ref, False) in val_set:\n",
    "                \n",
    "                _neg_example = None\n",
    "                \n",
    "                while(not _neg_example):\n",
    "                    _neg_id = random.randint(0,len(val_set))\n",
    "                    _neg_doi = get_doi(val_set[_neg_id])\n",
    "                    if (_neg_doi != _ref_doi) and (_neg_doi not in _ref_list):\n",
    "                        _neg_example = val_set[_neg_id]\n",
    "                \n",
    "#                 count+=1\n",
    "                yield (item, paper_list[_ref], _neg_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 1\n",
      "['10.1111/j.1467-8640.1988.tb00284.x', '10.1109/AAMAS.2004.129', '10.1007/978-3-540-30200-1_12', '10.1016/j.eswa.2011.01.045', '10.1109/IAT.2005.125', '10.1109/TNN.2007.905839', '10.1007/978-3-540-72383-7_128', '10.1007/BF00992698', '10.1016/S0734-189X(87)80014-2', '10.1364/ao.26.004919', '10.1109/TSMCB.2007.907040', '10.1007/978-3-642-38786-9_17', '10.1177/105971230000800302', '10.1080/0951508042000239048', '10.1016/0364-0213(90)90002-e', '10.1037/0096-3445.120.3.235', '10.1007/11553939_5', '7f60261da4f7e69951eb15bb4b8b8973a86e27e9', '10.1109/IAT.2006.100', '10.1109/ijcnn.2002.1007545', '10.1109/TNN.2004.826220', '10.1016/0893-6080(94)00092-Z', '10.1016/0893-6080(91)90056-B', '10.1007/bf00198465', '10.1109/WIIAT.2008.29']\n"
     ]
    }
   ],
   "source": [
    "iterat = get_triple(training_set)\n",
    "a1,a2,a3 = next(iterat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code in file tensor/two_layer_net_tensor.py\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ParallelTable(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ParallelTable, self).__init__()\n",
    "        n1 = nn.Linear(5, 2)\n",
    "        n2 = nn.Linear(5, 2)\n",
    "        n1.weight = n2.weight\n",
    "        n1.bias = n2.bias\n",
    "        \n",
    "        self.n1 = n1\n",
    "        self.n2 = n2\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.n1(x[0])\n",
    "        b = self.n2(x[1])\n",
    "        return (a,b)\n",
    "\n",
    "    \n",
    "class DistRes(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistRes, self).__init__()\n",
    "        self.layer1 = nn.Linear(20, 2)\n",
    "        self.layer2 = nn.PairwiseDistance(2)\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        res1 = self.layer1(x)\n",
    "        res2 = self.layer1(y)\n",
    "        return self.layer2(res1,res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prl = ParallelTable()\n",
    "mlp= nn.Sequential(prl, nn.PairwiseDistance(2))\n",
    "mlp2 = DistRes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4227  0.5567  0.8317  0.9628  0.5208\n",
       "[torch.FloatTensor of size 1x5]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "arg1 = Variable(torch.from_numpy(hv.transform([a1.abstract]).todense()))\n",
    "arg2 = Variable(torch.from_numpy(hv.transform([a2.abstract]).todense()))\n",
    "\n",
    "arg1 = Variable(torch.rand(1,5))\n",
    "arg2 = Variable(torch.rand(1,5))\n",
    "\n",
    "arg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.6993  0.1771\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prl([arg1,arg2])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = autograd.Variable(hv.transform([a1.abstract]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "k1 = hv.transform([a1.abstract])\n",
    "k2 = hv.transform([training_set[0].abstract])\n",
    "# for k in k2:\n",
    "#     print(k)\n",
    "np.sum(k1-k2)==0.0\n",
    "# (hv.transform([a1.abstract])==hv.transform([training_set[0].abstract])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_batch(iterable, n=100):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0,l,n):\n",
    "        yield iterable[ndx: min(ndx+n,l)]\n",
    "        \n",
    "# paper_list_full = list(paper_list.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract_training_set = [paper.abstract for paper in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract_training_set_hashed = hv.transform(abstract_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101 102 103]\n",
      " [105 106 107]]\n"
     ]
    }
   ],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 2**12 # 1st layer number of features\n",
    "n_hidden_2 = 2**8 # 2nd layer number of features\n",
    "n_input = N_FEATURES # MNIST data input (img shape: 28*28)\n",
    "n_classes = 2**6 # MNIST total classes (0-9 digits)\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "# Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "# Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "# Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "# Loop over all batches\n",
    "for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "# Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "# Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "# Display logs per epoch step\n",
    "if epoch % display_step == 0:\n",
    "print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "\"{:.9f}\".format(avg_cost))\n",
    "print(\"Optimization Finished!\")\n",
    "# Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}