{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import parser\n",
    "import numpy as np\n",
    "import gzip, msgpack\n",
    "from torch.nn.modules import PairwiseDistance\n",
    "import random\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import scipy.sparse as sp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "N_FEATURES = 2**18\n",
    "hv = HashingVectorizer(n_features=N_FEATURES,non_negative=True, stop_words='english')\n",
    "# try default n-gram 1-gram,2-gram,3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SmallReference:\n",
    "    def __init__(self, identifiers):\n",
    "        self.identifiers = identifiers\n",
    "\n",
    "class SmallPaper:\n",
    "    def __init__(self, title, identifiers, authors, abstract, refs):\n",
    "        self.title = title\n",
    "        self.authors = authors\n",
    "        self.abstract = hv.transform([abstract])\n",
    "        self.references = [SmallReference(ref.identifiers) for ref in refs]\n",
    "        self.identifiers = identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doi(paper):\n",
    "    _ref_dict = dict()\n",
    "    for x in paper.identifiers:\n",
    "#        @todo lowecase\n",
    "        _ref_dict[x.key_type] = x.key.lower()\n",
    "    if 'doi' in _ref_dict:\n",
    "        return _ref_dict['doi']\n",
    "    elif 'ieee' in _ref_dict:\n",
    "        return _ref_dict['ieee']\n",
    "    elif 'semanticscholar' in _ref_dict:\n",
    "        return _ref_dict['semanticscholar']\n",
    "    elif 'adsa' in _ref_dict:\n",
    "        return _ref_dict['adsa']\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def update_authors(ident_to_auth, authors_to_papers, paper):\n",
    "    for author in paper.authors:\n",
    "        author_name = author.name.name\n",
    "        \n",
    "        if not author_name:\n",
    "            continue\n",
    "        for identifier in author.identifiers:\n",
    "            result = ident_to_auth.get(identifier.key.lower(), False) \n",
    "            if result:\n",
    "                author_name = str(result)\n",
    "                break\n",
    "        for identifier in author.identifiers:\n",
    "            ident_to_auth[identifier.key.lower()] = author_name\n",
    "        if not authors_to_papers.get(author_name, False):\n",
    "            authors_to_papers[author_name] = list()\n",
    "        authors_to_papers[author_name].append(get_doi(paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:adsa:2000JMatS..35.4393W title is empty\n",
      "WARNING:root:adsa:2002RuPhJ..45..389B title is empty\n",
      "WARNING:root:doi:10.21609/jiki.v4i1.155 title is empty\n",
      "WARNING:root:adsa:2001JDDE...13....1K title is empty\n",
      "WARNING:root:doi:10.21609/jiki.v2i1.121 title is empty\n",
      "WARNING:root:doi:10.21609/jiki.v7i2.264 title is empty\n",
      "WARNING:root:doi:10.21609/jiki.v5i2.188 title is empty\n",
      "WARNING:root:adsa:2001JMatS..36.3113A title is empty\n",
      "WARNING:root:doi:10.21609/jiki.v9i1.287 title is empty\n",
      "WARNING:root:adsa:1998MPAG....1..107B title is empty\n",
      "WARNING:root:nature:10.1038/nrc1488 title is empty\n",
      "WARNING:root:adsa:1997JCAMD..11..135L title is empty\n",
      "WARNING:root:adsa:2012AGUFM.B53G..08B title is empty\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "identies_set = dict()\n",
    "\n",
    "_paper_dict = dict()\n",
    "myiter = 0\n",
    "count=0\n",
    "_ref_dict = dict()\n",
    "_training_set = list()\n",
    "_validation_set = list()\n",
    "_test_set = list()\n",
    "_ident_to_auth_dict = dict()\n",
    "_authors_to_papers_dict = dict()\n",
    "with gzip.open(\"NN_Papers.msgpack.gz\", \"rb\") as nn_papers_out:\n",
    "    unpacker = msgpack.Unpacker(nn_papers_out, encoding='utf-8')\n",
    "    for _paper in unpacker:\n",
    "        count+=1\n",
    "        paper = parser.Paper.deserialize(_paper)\n",
    "        \n",
    "        paper_doi = get_doi(paper)\n",
    "        if not (paper_doi and paper.abstract):\n",
    "            continue\n",
    "        else:\n",
    "            myiter+=1\n",
    "        paper = SmallPaper(paper.title, paper.identifiers, paper.authors, paper.abstract, paper.references)\n",
    "        if myiter>=200000:\n",
    "            break\n",
    "        #calculate statistics\n",
    "        for iden in paper.identifiers:\n",
    "            if identies_set.get(iden.key_type, False):\n",
    "                identies_set[iden.key_type]+=1\n",
    "            else:\n",
    "                identies_set[iden.key_type]=1\n",
    "        #end\n",
    "        \n",
    "        _ref_dict[paper_doi] = list()\n",
    " \n",
    "        update_authors(_ident_to_auth_dict, _authors_to_papers_dict, paper)\n",
    "        _paper_dict[paper_doi] = paper\n",
    "        if myiter<=160000:\n",
    "            _training_set.append(paper_doi)\n",
    "        elif myiter<=180000: \n",
    "            _test_set.append(paper_doi)\n",
    "        else:\n",
    "            _validation_set.append(paper_doi)\n",
    "\n",
    "print('total count: {0}'.format(count))\n",
    "print(identies_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_references(paper_dict, ref_dict):\n",
    "    count = 0\n",
    "    for paper_doi,paper in paper_dict.items():\n",
    "        for ref in paper.references:\n",
    "            ref_doi = get_doi(ref)\n",
    "            if not ref_doi:\n",
    "                continue\n",
    "            result = ref_dict.get(ref_doi, False)\n",
    "            if result!=False:\n",
    "                count+=1\n",
    "                ref_dict[paper_doi].append(ref_doi)\n",
    "    print(count)\n",
    "update_references(_paper_dict, _ref_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(paper_list.keys())[6]\n",
    "get_doi(_paper_dict['10.1109/tnnls.2015.2392563'].references[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract_training_set = [paper.abstract for paper in training_set]\n",
    "abstract_training_set_hashed = hv.transform(abstract_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_var(abstract):\n",
    "    dense = abstract.todense()\n",
    "    torch_tensor = Variable(torch.from_numpy(dense).type(dtype))\n",
    "    return torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "random.randint(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identies_set = dict()\n",
    "for paper in paper_list.values():\n",
    "    for iden in paper.identifiers:\n",
    "        if identies_set.get(iden.key_type, False):\n",
    "            identies_set[iden.key_type]+=1\n",
    "        else:\n",
    "            identies_set[iden.key_type]=1\n",
    "print(identies_set)\n",
    "len(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def get_triple(val_set):\n",
    "#     count = 0\n",
    "#     t0 = 0\n",
    "#     t1 =0 \n",
    "#     for item in val_set:\n",
    "#         count += 1\n",
    "#         _ref_doi = get_doi(item)\n",
    "#         if _ref_doi not in ref_dict.keys():\n",
    "#             continue\n",
    "#         _ref_list = ref_dict[_ref_doi]\n",
    "        \n",
    "#         for _ref in _ref_list:\n",
    "#             if paper_list.get(_ref, False) in val_set:\n",
    "#                 _neg_example = None\n",
    "#                 while(not _neg_example):\n",
    "#                     _neg_id = random.randint(0,len(val_set)-1)\n",
    "#                     _neg_doi = get_doi(val_set[_neg_id])\n",
    "#                     if (_neg_doi != _ref_doi) and (_neg_doi not in _ref_list):\n",
    "#                         neg_ref_dict = ref_dict[_neg_doi]\n",
    "#                         if not (_ref_doi in neg_ref_dict or _ref in neg_ref_dict):\n",
    "#                             _neg_example = val_set[_neg_id]\n",
    "# #                 t1 = time.time()\n",
    "# #                 print('time to sample triple: {0}'.format(t1-t0))\n",
    "#                 yield (item.abstract, paper_list[_ref].abstract, _neg_example.abstract)\n",
    "# #                 t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "def prepare_triples(val_set):\n",
    "    count = 0\n",
    "    result_list = list()\n",
    "    batch_list_1 = None\n",
    "    batch_list_2 = None\n",
    "    batch_list_3 = None\n",
    "    print(len(val_set))\n",
    "    for item in val_set:\n",
    "        _ref_doi = get_doi(item)\n",
    "        if _ref_doi not in ref_dict.keys():\n",
    "            continue\n",
    "        _ref_list = ref_dict[_ref_doi]\n",
    "        if len(_ref_list) == 0:\n",
    "            print(get_doi(item))\n",
    "        for _ref in _ref_list:\n",
    "            \n",
    "            if paper_list.get(_ref, False) in val_set:\n",
    "                _neg_example = None\n",
    "                while(not _neg_example):\n",
    "                    _neg_id = random.randint(0,len(val_set)-1)\n",
    "                    _neg_doi = get_doi(val_set[_neg_id])\n",
    "                    if (_neg_doi != _ref_doi) and (_neg_doi not in _ref_list):\n",
    "                        neg_ref_dict = ref_dict[_neg_doi]\n",
    "                        if not (_ref_doi in neg_ref_dict or _ref in neg_ref_dict):\n",
    "                            _neg_example = val_set[_neg_id]\n",
    "                            count+=1\n",
    "                \n",
    "                if batch_list_1 == None:\n",
    "                    batch_list_1 = item.abstract\n",
    "                    batch_list_2 = paper_list[_ref].abstract\n",
    "                    batch_list_3 = _neg_example.abstract\n",
    "                else:\n",
    "                    batch_list_1 = sp.vstack((batch_list_1, item.abstract))\n",
    "                    batch_list_2 = sp.vstack((batch_list_2, paper_list[_ref].abstract))\n",
    "                    batch_list_3 = sp.vstack((batch_list_3, _neg_example.abstract))\n",
    "                \n",
    "                if batch_list_1.shape[0]==BATCH_SIZE:\n",
    "                    result_list.append((batch_list_1, batch_list_2, batch_list_3))\n",
    "                    batch_list_1 = None\n",
    "    print(count)\n",
    "    return result_list\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_list = prepare_triples(training_set[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_dict['10.1117/12.24153']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "def get_batch(iterator, batch_size=BATCH_SIZE):\n",
    "    a1 = None\n",
    "    a2 = None\n",
    "    a3 = None\n",
    "    for x1,x2,x3 in iterator:\n",
    "        if a1 == None:\n",
    "            a1=x1\n",
    "            a2=x2\n",
    "            a3=x3\n",
    "        else:\n",
    "            a1 = sp.vstack((a1,x1))\n",
    "            a2 = sp.vstack((a2,x2))\n",
    "            a3 = sp.vstack((a3,x3))            \n",
    "        if a1.shape[0]==batch_size:\n",
    "            dense_a1 = a1.todense()\n",
    "            dense_a2 = a2.todense()\n",
    "            dense_a3 = a3.todense()\n",
    "            tor_a1 = torch.from_numpy(dense_a1).type(dtype)\n",
    "            tor_a2 = torch.from_numpy(dense_a2).type(dtype)\n",
    "            tor_a3 = torch.from_numpy(dense_a3).type(dtype)\n",
    "            yield (Variable(tor_a1),\n",
    "                   Variable(tor_a2),\n",
    "                   Variable(tor_a3))\n",
    "            a1 = None\n",
    "            a2 = None\n",
    "            a3 = None\n",
    "\n",
    "\n",
    "# input data sparse matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tri_iter = get_triple(training_set)\n",
    "x1,x2,x3 = next(tri_iter)\n",
    "# x1 = sp.vstack((x1,x2),dtype=dtype)\n",
    "# x1.todense()\n",
    "\n",
    "batch_iterator = get_batch(tri_iter)\n",
    "kk1,kk2,kk3 = next(batch_iterator)\n",
    "kk1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hv.transform(['avasd', 'asdfwe sdfasdfwe werwe adsfsd']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistRes(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistRes, self).__init__()\n",
    "        self.layer1 = nn.Linear(N_FEATURES, 100)\n",
    "        self.layer2 = nn.PairwiseDistance(2)\n",
    "\n",
    "    def forward(self, x1,x2):\n",
    "#         print('typex: {0}'.format(type(x1)))\n",
    "#         print('typey: {0}'.format(type(x2)))\n",
    "        res1 = self.layer1(x1)\n",
    "        res2 = self.layer1(x2)\n",
    "        # normalize vector L2-normalization\n",
    "        return self.layer2(res1,res2)\n",
    "    \n",
    "# linear/RelU/linear\n",
    "# dropout input layer\n",
    "# help as they authored by same person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dist_model = DistRes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# arg1 = Variable(torch.from_numpy(hv.transform([a1.abstract]).todense()))\n",
    "# arg2 = Variable(torch.from_numpy(hv.transform([a2.abstract]).todense()))\n",
    "\n",
    "# arg1 = Variable(torch.rand(2,5), requires_grad=True)\n",
    "# arg2 = Variable(torch.rand(2,5), requires_grad=True)\n",
    "# arg3 = Variable(torch.rand(2,5), requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check the distance Loss, just to see the difference\n",
    "# maybe pos and pos close\n",
    "\n",
    "crit = nn.HingeEmbeddingLoss(1)\n",
    "optim = torch.optim.SGD(dist_model.parameters(), lr=1e-3)\n",
    "# local_dist(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones_var = Variable(torch.ones(BATCH_SIZE))\n",
    "neg_var = Variable(torch.ones(BATCH_SIZE)*(-1))\n",
    "\n",
    "def train_networks(model, criteria, optimizer, paper, p_pos, p_neg):\n",
    "                    \n",
    "    loss_pos = criteria.forward( model(paper, p_pos), ones_var)\n",
    "    optimizer.zero_grad()\n",
    "    gradCrit = loss_pos.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_neg = criteria.forward( model(paper,p_neg), neg_var)\n",
    "    optimizer.zero_grad()\n",
    "    gradCrit = loss_neg.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return model(paper, p_pos), model(paper,p_neg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    aver_pos = list()\n",
    "    aver_neg = list()\n",
    "    triple_iterator = get_triple(training_set)\n",
    "    batch_iterator = get_batch(triple_iterator)\n",
    "    batch=0\n",
    "    t0=0\n",
    "    t1=0\n",
    "    for arg1,arg2,arg3 in batch_iterator:\n",
    "        t0 = time.time()\n",
    "        print('sampling time: {0}'.format(t0-t1))\n",
    "        ap, an = train_networks(model=dist_model, criteria=crit, optimizer=optim, paper=arg1, p_pos=arg2, p_neg=arg3)\n",
    "        t1 = time.time()\n",
    "        total= t1-t0\n",
    "        aver_pos.append(ap.data.mean())\n",
    "        aver_neg.append(an.data.mean())\n",
    "        print('Epoch: {0}. Batch: {3}. Time {4} av_pos: {1}. av_neg: {2}'.format(i, np.mean(aver_pos), \n",
    "                                                                                 np.mean(aver_neg), \n",
    "                                                                                 batch, total))\n",
    "        batch+=1\n",
    "    print('Epoch: {0}. av_pos: {1}. av_neg: {2}'.format(i, np.mean(aver_pos), np.mean(aver_neg)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triple_iterator1 = get_triple(training_set)\n",
    "batch_iterator1 = get_batch(triple_iterator1)\n",
    "arg1,arg2,arg3 = next(batch_iterator1)\n",
    "k = dist_model(arg1,arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def calculate_distances(model,data_set):\n",
    "\n",
    "#     stacked_matrix = data_set[0].abstract\n",
    "#     for item in data_set[1:]:\n",
    "#         stacked_matrix = sp.vstack((stacked_matrix, item.abstract))\n",
    "        \n",
    "#     res = model.layer1.forward(make_var(stacked_matrix)).data.numpy()\n",
    "#     return res\n",
    "\n",
    "# stacked_matrix = calculate_distances(dist_model, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
