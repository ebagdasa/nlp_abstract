{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import parser\n",
    "import numpy as np\n",
    "import gzip, msgpack\n",
    "from torch.nn.modules import PairwiseDistance\n",
    "import random\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import scipy.sparse as sp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "N_FEATURES = 100\n",
    "hv = HashingVectorizer(n_features=N_FEATURES,non_negative=True, analyzer='word',\n",
    "                       ngram_range=(2,3), norm='l2', stop_words='english')\n",
    "# try default n-gram 1-gram,2-gram,3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SmallReference:\n",
    "    def __init__(self, identifiers):\n",
    "        self.identifiers = identifiers\n",
    "\n",
    "class SmallPaper:\n",
    "    def __init__(self, title, identifiers, authors, abstract, refs):\n",
    "        self.title = title\n",
    "        self.authors = authors\n",
    "        self.abstract = hv.transform([abstract])\n",
    "        self.references = [SmallReference(ref.identifiers) for ref in refs]\n",
    "        self.identifiers = identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doi(paper):\n",
    "    _ref_dict = dict()\n",
    "    for x in paper.identifiers:\n",
    "#        @todo lowecase\n",
    "        _ref_dict[x.key_type] = x.key.lower()\n",
    "    if 'doi' in _ref_dict:\n",
    "        return _ref_dict['doi']\n",
    "    elif 'ieee' in _ref_dict:\n",
    "        return _ref_dict['ieee']\n",
    "    elif 'semanticscholar' in _ref_dict:\n",
    "        return _ref_dict['semanticscholar']\n",
    "    elif 'adsa' in _ref_dict:\n",
    "        return _ref_dict['adsa']\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def update_authors(ident_to_auth, authors_to_papers, paper):\n",
    "    for author in paper.authors:\n",
    "        author_name = author.name.name\n",
    "        \n",
    "        if not author_name:\n",
    "            continue\n",
    "        for identifier in author.identifiers:\n",
    "            result = ident_to_auth.get(identifier.key.lower(), False) \n",
    "            if result:\n",
    "                author_name = str(result)\n",
    "                break\n",
    "        for identifier in author.identifiers:\n",
    "            ident_to_auth[identifier.key.lower()] = author_name\n",
    "        if not authors_to_papers.get(author_name, False):\n",
    "            authors_to_papers[author_name] = list()\n",
    "        authors_to_papers[author_name].append(get_doi(paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:adsa:2000JMatS..35.4393W title is empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total count: 2880\n",
      "{'acm': 397, 'doi': 1518, 'springer': 180, 'onepetro': 8, 'adsa': 451, 'oxford': 53, 'science': 2, 'ieee': 666, 'nature': 26, 'semanticscholar': 544, 'iop': 35, 'pmid': 257, 'mid': 5, 'pmc': 73, 'sage': 11, 'arxiv': 103, 'manuscript': 8}\n"
     ]
    }
   ],
   "source": [
    "MAX_ELEMENTS = 2000\n",
    "train_test_val_share = [0.8*MAX_ELEMENTS, 0.9*MAX_ELEMENTS, MAX_ELEMENTS]\n",
    "\n",
    "identies_set = dict()\n",
    "\n",
    "_paper_dict = dict()\n",
    "myiter = 0\n",
    "count=0\n",
    "_ref_dict = dict()\n",
    "_training_set = list()\n",
    "_validation_set = list()\n",
    "_test_set = list()\n",
    "_ident_to_auth_dict = dict()\n",
    "_authors_to_papers_dict = dict()\n",
    "with gzip.open(\"NN_Papers.msgpack.gz\", \"rb\") as nn_papers_out:\n",
    "    unpacker = msgpack.Unpacker(nn_papers_out, encoding='utf-8')\n",
    "    for _paper in unpacker:\n",
    "        count+=1\n",
    "        paper = parser.Paper.deserialize(_paper)\n",
    "        \n",
    "        paper_doi = get_doi(paper)\n",
    "        if not (paper_doi and paper.abstract):\n",
    "            continue\n",
    "        else:\n",
    "            myiter+=1\n",
    "        paper = SmallPaper(paper.title, paper.identifiers, paper.authors, paper.abstract, paper.references)\n",
    "        if myiter>=MAX_ELEMENTS:\n",
    "            break\n",
    "        #calculate statistics\n",
    "        for iden in paper.identifiers:\n",
    "            if identies_set.get(iden.key_type, False):\n",
    "                identies_set[iden.key_type]+=1\n",
    "            else:\n",
    "                identies_set[iden.key_type]=1\n",
    "        #end\n",
    "        \n",
    "        _ref_dict[paper_doi] = list()\n",
    " \n",
    "        update_authors(_ident_to_auth_dict, _authors_to_papers_dict, paper)\n",
    "        _paper_dict[paper_doi] = paper\n",
    "        if myiter<=train_test_val_share[0]:\n",
    "            _training_set.append(paper_doi)\n",
    "        elif myiter<=train_test_val_share[1]: \n",
    "            _test_set.append(paper_doi)\n",
    "        else:\n",
    "            _validation_set.append(paper_doi)\n",
    "\n",
    "print('total count: {0}'.format(count))\n",
    "print(identies_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "def update_references(paper_dict, ref_dict):\n",
    "    count = 0\n",
    "    for paper_doi,paper in paper_dict.items():\n",
    "        for ref in paper.references:\n",
    "            ref_doi = get_doi(ref)\n",
    "            if not ref_doi:\n",
    "                continue\n",
    "            result = ref_dict.get(ref_doi, False)\n",
    "            if result!=False:\n",
    "                count+=1\n",
    "                ref_dict[paper_doi].append(ref_doi)\n",
    "    print(count)\n",
    "update_references(_paper_dict, _ref_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for key, value in _authors_to_papers_dict.items():\n",
    "#     if len(value)>20:\n",
    "#         count+=1\n",
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(_authors_to_papers_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(list(_paper_dict.values())[11].abstract)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# abstract_training_set = [paper.abstract for paper in training_set]\n",
    "# abstract_training_set_hashed = hv.transform(abstract_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "\n",
    "def make_var(abstract):\n",
    "    dense = abstract.todense()\n",
    "    torch_tensor = Variable(torch.from_numpy(dense).type(dtype))\n",
    "    return torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# random.randint(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# identies_set = dict()\n",
    "# for paper in paper_list.values():\n",
    "#     for iden in paper.identifiers:\n",
    "#         if identies_set.get(iden.key_type, False):\n",
    "#             identies_set[iden.key_type]+=1\n",
    "#         else:\n",
    "#             identies_set[iden.key_type]=1\n",
    "# print(identies_set)\n",
    "# len(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def get_triple(val_set):\n",
    "#     count = 0\n",
    "#     t0 = 0\n",
    "#     t1 =0 b\n",
    "#     for item in val_set:\n",
    "#         count += 1\n",
    "#         _ref_doi = get_doi(item)\n",
    "#         if _ref_doi not in ref_dict.keys():\n",
    "#             continue\n",
    "#         _ref_list = ref_dict[_ref_doi]\n",
    "        \n",
    "#         for _ref in _ref_list:\n",
    "#             if paper_list.get(_ref, False) in val_set:\n",
    "#                 _neg_example = None\n",
    "#                 while(not _neg_example):\n",
    "#                     _neg_id = random.randint(0,len(val_set)-1)\n",
    "#                     _neg_doi = get_doi(val_set[_neg_id])\n",
    "#                     if (_neg_doi != _ref_doi) and (_neg_doi not in _ref_list):\n",
    "#                         neg_ref_dict = ref_dict[_neg_doi]\n",
    "#                         if not (_ref_doi in neg_ref_dict or _ref in neg_ref_dict):\n",
    "#                             _neg_example = val_set[_neg_id]\n",
    "# #                 t1 = time.time()\n",
    "# #                 print('time to sample triple: {0}'.format(t1-t0))\n",
    "#                 yield (item.abstract, paper_list[_ref].abstract, _neg_example.abstract)\n",
    "# #                 t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# appr = _paper_dict['10.1109/tnnls.2015.2392563']\n",
    "def get_papers_per_authors(paper):\n",
    "    set_to_merge = set()\n",
    "    for author in paper.authors:\n",
    "        identifier = list(author.identifiers)[0].key\n",
    "#         print(\"name {0}. papers: {1}\".format(_ident_to_auth_dict[identifier],\n",
    "#                                              _authors_to_papers_dict[_ident_to_auth_dict[identifier]]))\n",
    "        set_to_merge.update(set(_authors_to_papers_dict[_ident_to_auth_dict[identifier.lower()]]))\n",
    "    set_to_merge.discard(get_doi(paper))\n",
    "    return set_to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_papers_per_authors(_paper_dict['10.1109/tnnls.2015.2392563'])\n",
    "# [x for x in (1,2,3,4)]\n",
    "# {1,3,4,5}.difference(_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "\n",
    "def prepare_triples(val_set):\n",
    "    count = 0\n",
    "    result_list = list()\n",
    "    batch_list_1 = None\n",
    "    batch_list_2 = None\n",
    "    batch_list_3 = None\n",
    "#     print(len(val_set))\n",
    "    for paper_doi in val_set:\n",
    "        paper = _paper_dict[paper_doi]\n",
    "        \n",
    "        \n",
    "        pos_ref_set = get_papers_per_authors(paper)\n",
    "        \n",
    "        pos_ref_set.update(_ref_dict[paper_doi])\n",
    "        pos_ref_set = (pos_ref_set.difference(_validation_set)).difference(_test_set)\n",
    "\n",
    "        #cut down articles that are not used in val_set\n",
    "        \n",
    "#         pos_ref_set.intersection(val_set)\n",
    "        \n",
    "        if len(pos_ref_set) == 0:\n",
    "#             print(paper_doi)\n",
    "            continue\n",
    "#         print(len(pos_ref_set))\n",
    "        neg_ref_set = set()\n",
    "        for n in range(len(pos_ref_set)):\n",
    "            new_id = None\n",
    "            while((new_id in pos_ref_set) or new_id==None):\n",
    "                new_id = val_set[random.randrange(0,len(val_set)-1)]\n",
    "            neg_ref_set.add(new_id)\n",
    "            \n",
    "            \n",
    "        for pos,neg in zip(pos_ref_set, neg_ref_set):\n",
    "\n",
    "            if batch_list_1 == None:\n",
    "                batch_list_1 = paper.abstract\n",
    "                batch_list_2 = _paper_dict[pos].abstract\n",
    "                batch_list_3 = _paper_dict[neg].abstract\n",
    "            else:\n",
    "                batch_list_1 = sp.vstack((batch_list_1, paper.abstract))\n",
    "                batch_list_2 = sp.vstack((batch_list_2, _paper_dict[pos].abstract))\n",
    "                batch_list_3 = sp.vstack((batch_list_3, _paper_dict[neg].abstract))\n",
    "\n",
    "            if batch_list_1.shape[0]==BATCH_SIZE:\n",
    "                yield (batch_list_1, batch_list_2, batch_list_3)\n",
    "                batch_list_1 = None\n",
    "#     print(count)\n",
    "    return result_list\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# get_papers_per_authors(_paper_dict[_training_set[2]]).intersection(_training_set[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a1,a2,a3 = next(triples_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# dtype = torch.FloatTensor\n",
    "# def get_batch(iterator, batch_size=BATCH_SIZE):\n",
    "#     a1 = None\n",
    "#     a2 = None\n",
    "#     a3 = None\n",
    "#     for x1,x2,x3 in iterator:\n",
    "#         if a1 == None:\n",
    "#             a1=x1\n",
    "#             a2=x2\n",
    "#             a3=x3\n",
    "#         else:\n",
    "#             a1 = sp.vstack((a1,x1))\n",
    "#             a2 = sp.vstack((a2,x2))\n",
    "#             a3 = sp.vstack((a3,x3))            \n",
    "#         if a1.shape[0]==batch_size:\n",
    "#             dense_a1 = a1.todense()\n",
    "#             dense_a2 = a2.todense()\n",
    "#             dense_a3 = a3.todense()\n",
    "#             tor_a1 = torch.from_numpy(dense_a1).type(dtype)\n",
    "#             tor_a2 = torch.from_numpy(dense_a2).type(dtype)\n",
    "#             tor_a3 = torch.from_numpy(dense_a3).type(dtype)\n",
    "#             yield (Variable(tor_a1),\n",
    "#                    Variable(tor_a2),\n",
    "#                    Variable(tor_a3))\n",
    "#             a1 = None\n",
    "#             a2 = None\n",
    "#             a3 = None\n",
    "\n",
    "\n",
    "# input data sparse matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hv.transform(['avasd', 'asdfwe sdfasdfwe werwe adsfsd']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistRes2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistRes, self).__init__()\n",
    "        self.layer1 = nn.Linear(N_FEATURES, 50)\n",
    "        self.layer2 = nn.PairwiseDistance(2)\n",
    "\n",
    "    def forward(self, x1,x2):\n",
    "#         print('typex: {0}'.format(type(x1)))\n",
    "#         print('typey: {0}'.format(type(x2)))\n",
    "        res1 = self.layer1(x1)\n",
    "        res2 = self.layer1(x2)\n",
    "        # normalize vector L2-normalization\n",
    "        return self.layer2(res1,res2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistRes(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistRes, self).__init__()\n",
    "        self.layer1 = nn.Linear(N_FEATURES, 50)\n",
    "        self.layer2 = nn.PairwiseDistance(2)\n",
    "\n",
    "    def forward(self, x1,x2):\n",
    "#         print('typex: {0}'.format(type(x1)))\n",
    "#         print('typey: {0}'.format(type(x2)))\n",
    "        res1 = torch.nn.functional.tanh(self.layer1(x1))\n",
    "        res2 = torch.nn.functional.tanh(self.layer1(x2))\n",
    "\n",
    "#         res3 = self.layer1(x3)\n",
    "        # normalize vector L2-normalization\n",
    "        return torch.nn.functional.tanh(self.layer2(res1,res2))\n",
    "    \n",
    "    def calculate_res(self, x):\n",
    "        return torch.nn.functional.tanh(self.layer1(x)).data.numpy()\n",
    "    \n",
    "# linear/RelU/linear\n",
    "# dropout input layer\n",
    "# help as they authored by same person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dist_model = DistRes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check the distance Loss, just to see the difference\n",
    "# maybe pos and pos close\n",
    "\n",
    "crit = nn.HingeEmbeddingLoss(1)\n",
    "optim = torch.optim.RMSprop(dist_model.parameters(), momentum=0.5, lr=0.01)\n",
    "# local_dist(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones_var = Variable(torch.ones(BATCH_SIZE))\n",
    "neg_var = Variable(torch.ones(BATCH_SIZE)*(-1))\n",
    "\n",
    "def train_networks(model, criteria, optimizer, paper, p_pos, p_neg):\n",
    "\n",
    "    loss_pos = criteria.forward( model(paper, p_pos), ones_var)\n",
    "    optimizer.zero_grad()\n",
    "    gradCrit = loss_pos.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_neg = criteria.forward( model(paper,p_neg), neg_var)\n",
    "    optimizer.zero_grad()\n",
    "    gradCrit = loss_neg.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return model(paper, p_pos), model(paper,p_neg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "RECALL_N = 20\n",
    "\n",
    "def calculate_recall(model, val_set, train_set):\n",
    "#     if len(val_set)<1800:\n",
    "#         raise ValueError('The set is too small to test.')\n",
    "    a=0\n",
    "    recall_at_n = list()\n",
    "    \n",
    "    #calculate all vectors for the set\n",
    "    doi_to_vecs = dict()\n",
    "    count_1 = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for paper_doi in val_set[:500]:\n",
    "        paper = _paper_dict[paper_doi]\n",
    "        pos_ref_set = get_papers_per_authors(paper)\n",
    "        pos_ref_set.update(_ref_dict[paper_doi])\n",
    "\n",
    "\n",
    "        \n",
    "        if len(pos_ref_set)==0 or len(pos_ref_set)>RECALL_N:\n",
    "            continue\n",
    "        if len(pos_ref_set)>1:\n",
    "            count_1+=1\n",
    "        neg_examples_amount = 200\n",
    "        neg_ref_set = set()\n",
    "        for n in range(neg_examples_amount):\n",
    "            new_id = None\n",
    "            while((new_id in pos_ref_set) or new_id==None or (new_id in neg_ref_set)):\n",
    "                new_id = train_set[random.randrange(0,len(train_set))]\n",
    "            neg_ref_set.add(new_id)\n",
    "        \n",
    "        all_refs = set()\n",
    "        all_refs.update(pos_ref_set)\n",
    "        all_refs.update(neg_ref_set)\n",
    "\n",
    "        \n",
    "        for paper_doi in all_refs:\n",
    "            doi_to_vecs[paper_doi] = model.calculate_res(make_var(_paper_dict[paper_doi].abstract))\n",
    "        \n",
    "        paper_vector = doi_to_vecs[paper_doi]\n",
    "        \n",
    "        \n",
    "        #calculate\n",
    "        distance_array = dict()\n",
    "        for ref in all_refs: \n",
    "            ref_vec = doi_to_vecs[ref]\n",
    "            ref_dist = np.linalg.norm(paper_vector - ref_vec)\n",
    "            distance_array[ref] = ref_dist\n",
    "\n",
    "        ordered_predicted_list = list(map(lambda x: x[0], \n",
    "                                          sorted(distance_array.items(), \n",
    "                                                 key=lambda val: val[1])))[:RECALL_N]\n",
    "        recall = len(pos_ref_set.intersection(ordered_predicted_list)) / len(pos_ref_set)\n",
    "        if a!=0:\n",
    "            print(ordered_predicted_list)\n",
    "            print(pos_ref_set)\n",
    "            a-=1\n",
    "        #print(recall)\n",
    "        recall_at_n.append(recall)\n",
    "#     print(count_1)\n",
    "    return np.mean(recall_at_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = time.time()\n",
    "# print(calculate_recall(model=dist_model, val_set=valid_set))\n",
    "# print(time.time()-t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'10.1023/a:1007989231205' in valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = _training_set\n",
    "valid_set = _training_set\n",
    "# t0=time.time()\n",
    "\n",
    "# print(time.time()-t0)\n",
    "# print(len(triples_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_time = time.time()\n",
    "\n",
    "def write_to_file(file, text_to_write):\n",
    "    file.write(text_to_write)\n",
    "    file.write('\\n')\n",
    "    file.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 0. av_pos: 0.8776989781931261. av_neg: 0.9496426434516906. recall: 0.09738955823293172\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 1. av_pos: 0.8476370779946665. av_neg: 0.9648127841949463. recall: 0.06626506024096386\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "epoch_auc = dict()\n",
    "my_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "f = open('/home/jupyter/output_{0}.txt'.format(my_time),'w')\n",
    "\n",
    "\n",
    "recall = calculate_recall(model=dist_model, val_set=valid_set, train_set=train_set)\n",
    "text = '\\nBeginning recall: {0}\\n'.format(recall)\n",
    "print(text)\n",
    "write_to_file(f, text)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    aver_pos = list()\n",
    "    aver_neg = list()\n",
    "    triples_iterator = prepare_triples(train_set)    \n",
    "    batch=0\n",
    "    t0 = time.time()\n",
    "    for arg1,arg2,arg3 in triples_iterator:\n",
    "\n",
    "        ap, an = train_networks(model=dist_model, criteria=crit, optimizer=optim, \n",
    "                                paper=make_var(arg1), p_pos=make_var(arg2), \n",
    "                                p_neg=make_var(arg3))\n",
    "        \n",
    "        aver_pos.append(ap.data.mean())\n",
    "        aver_neg.append(an.data.mean())\n",
    "        if (batch % 100 == 0 and batch != 0 ):\n",
    "            t1 = time.time()\n",
    "            total= t1-t0\n",
    "            text = 'Epoch: {0}. Batch: {3}. Time {4} av_pos: {1}. av_neg: {2}'.format(epoch, np.mean(aver_pos), \n",
    "                                                                               np.mean(aver_neg), \n",
    "                                                                                 batch, total)\n",
    "            print(text)\n",
    "            write_to_file(f, text)\n",
    "            t0 = time.time()\n",
    "        batch+=1\n",
    "    \n",
    "    #calc auc\n",
    "    t0 = time.time()\n",
    "    recall = calculate_recall(model=dist_model, val_set=valid_set, train_set=train_set)\n",
    "    t1 = time.time()\n",
    "#     print('recall calculated in {0} s.'.format(t1-t0))\n",
    "    epoch_auc[epoch] = recall\n",
    "    text = '\\n\\nEpoch: {0}. av_pos: {1}. av_neg: {2}. recall: {3}\\n\\n'.format(epoch, np.mean(aver_pos), np.mean(aver_neg), recall)\n",
    "    write_to_file(f, text)\n",
    "    print(text)\n",
    "\n",
    "f.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triple_iterator1 = get_triple(training_set)\n",
    "batch_iterator1 = get_batch(triple_iterator1)\n",
    "arg1,arg2,arg3 = next(batch_iterator1)\n",
    "k = dist_model(arg1,arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = {'a':1,'b':5, 'c':3,'d':0}\n",
    "b = list(map(lambda x: x[0], sorted(a.items(), key=lambda val: val[1])[:3]))\n",
    "c = {'d', 'a'}\n",
    "d = len(c.intersection(b))/len(c)\n",
    "np.mean([d,0])\n",
    "int(10 / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
